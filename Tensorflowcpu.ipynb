{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/c/digit-recognizer/submit\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "%matplotlib inline\n",
    "\n",
    "# create the training & test sets, skipping the header row with [1:]\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "xtrain = train.drop('label',axis=1).values.reshape((42000,28,28,1))/ 255.0\n",
    "ytrain = train['label'].values\n",
    "# Maybe need to use trainLabels = np_utils.to_categorical(trainLabels, 10)\n",
    "xtest = test.values.reshape((28000,28,28,1))/ 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# http://www.pyimagesearch.com/2016/08/01/lenet-convolutional-neural-network-in-python/\n",
    "# http://deeplearning.net/tutorial/lenet.html\n",
    "# https://arxiv.org/pdf/1606.02228v2.pdf\n",
    "\n",
    "# conv layer argument: stride, size, padding\n",
    "# stride is how much the filter moves. \n",
    "# With no padding, a stride of 1 and a size nxn, the size of the output is reduce by n-1.\n",
    "# With padding and a stride of 1, the size is not reduced.\n",
    "# Stide bigger than 1 reduce the size of the output.\n",
    "# O = (I - K + P)/S + 1\n",
    "# O: output size, I = input size, K = filter size, P is the padding, S is the stride.\n",
    "# The 2 padding choice in keras are valid (no padding) and same (output as the same size as input)\n",
    "# It seems strange that same would be O = I for stride different than 1 (as it would add purely 0 result). Is it an error of the \n",
    "# documentation?\n",
    "# Someone say than \"same\" means that for filter size k, the padding is round down k/2 on the RHS and round up k/2 on the LHS.\n",
    "# By default stride is 1.\n",
    "# It doesn't seem useful to have a different stride for the convolution since the reduction of the size can be done my pooling\n",
    "# Pooling should be done with a stride of same size as the kernel size.\n",
    "\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "model = Sequential()\n",
    "# 2D convolution filters, where each filter is of size 5 x 5\n",
    "# 28 x 28 inputs with a single channel for depth \n",
    "# padding=\"same\" means that padding is done such that the output as the same size as the input.\n",
    "model.add(Conv2D(filters=20, kernel_size=5, padding=\"same\", input_shape=(28, 28, 1)))\n",
    "model.add(Activation(\"relu\"))\n",
    "# 2 x 2 max-pooling moving by step of 2 in both directions. Reduce shape to 14 x 14\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "# 50 convolution filters, where each filter is of size 5 x 5\n",
    "model.add(Conv2D(filters=50, kernel_size=5, padding=\"same\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "# Reduce shape to 7 x 7 with 50 channels\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "# Fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500))\n",
    "model.add(Activation(\"relu\"))\n",
    "# softmax classifier  For 10 results\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 28, 28, 1)\n",
      "(42000, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "label = to_categorical(ytrain, num_classes=10)\n",
    "print(xtrain.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to execute this if I don't execute the following cells.\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import argmax\n",
    "# Split into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xtrain, label, test_size=0.2, random_state=42)\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28, 1)\n",
      "(1000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Use only the first 1000\n",
    "xtrain0 = xtrain[:1000]\n",
    "print(xtrain0.shape)\n",
    "label0 = label[:1000]\n",
    "print(label0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3s - loss: 1.0754 - acc: 0.6560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x201154ea048>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xtrain0, label0, epochs=1, batch_size=32,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 10)\n",
      "(1000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.82899999999999996"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import argmax\n",
    "\n",
    "x1 = xtrain[1000:2000]\n",
    "y1 = ytrain[1000:2000]\n",
    "\n",
    "ans = model.predict(x1)\n",
    "print(ans.shape)\n",
    "ans = argmax(ans, axis=-1)\n",
    "print(ans.shape)\n",
    "\n",
    "accuracy_score(y1, ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "52s - loss: 0.0446 - acc: 0.9863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.98642857142857143"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xtrain, label, test_size=0.2, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=1, batch_size=32,verbose=2)\n",
    "ans = argmax(model.predict(X_test), axis=-1)\n",
    "truth = argmax(y_test, axis=-1)\n",
    "accuracy_score(truth, ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save result for Kaggle test, to get the Kaggle test score.\n",
    "\n",
    "ans = argmax(model.predict(xtest), axis=-1)\n",
    "ans_s = pd.Series(data=ans,index=range(1,28001)).rename('Label')\n",
    "ans_s.to_csv(path='Result0',header=True,index_label='ImageId')\n",
    "\n",
    "# I scored 0.98500, that is 420 mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "54s - loss: 0.0332 - acc: 0.9900\n",
      "Epoch 2/5\n",
      "52s - loss: 0.0247 - acc: 0.9928\n",
      "Epoch 3/5\n",
      "52s - loss: 0.0193 - acc: 0.9938\n",
      "Epoch 4/5\n",
      "51s - loss: 0.0181 - acc: 0.9954\n",
      "Epoch 5/5\n",
      "51s - loss: 0.0146 - acc: 0.9960\n",
      "0.986547619048\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=5, batch_size=32,verbose=2)\n",
    "ans = argmax(model.predict(X_test), axis=-1)\n",
    "truth = argmax(y_test, axis=-1)\n",
    "print(accuracy_score(truth, ans))\n",
    "\n",
    "\n",
    "# Save result for Kaggle test, to get the Kaggle test score.\n",
    "\n",
    "ans = argmax(model.predict(xtest), axis=-1)\n",
    "ans_s = pd.Series(data=ans,index=range(1,28001)).rename('Label')\n",
    "ans_s.to_csv(path='Result1',header=True,index_label='ImageId')\n",
    "\n",
    "# I scored 0.98514, that is 416 mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/5\n",
      "54s - loss: 0.0132 - acc: 0.9965 - val_loss: 0.0715 - val_acc: 0.9871\n",
      "Epoch 2/5\n",
      "54s - loss: 0.0133 - acc: 0.9964 - val_loss: 0.0465 - val_acc: 0.9912\n",
      "Epoch 3/5\n",
      "54s - loss: 0.0109 - acc: 0.9973 - val_loss: 0.0535 - val_acc: 0.9907\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1df00847d68>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "# val_acc or val_loss\n",
    "stop = [EarlyStopping(monitor='val_acc', min_delta=0, patience=0)]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32,verbose=2,callbacks=stop,shuffle=True, \n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "# My val_acc is 0.9912, my test_acc should be of this order, but it is likely to be slightly lower because I picked this\n",
    "# val_acc because it's the biggest one which is a bias.\n",
    "# 0.9912 is significantly better than 0.98514 my previous result even if we consider that the real result might be slightly lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/100\n",
      "56s - loss: 0.0168 - acc: 0.9954 - val_loss: 0.0517 - val_acc: 0.9892\n",
      "Epoch 2/100\n",
      "57s - loss: 0.0141 - acc: 0.9965 - val_loss: 0.0551 - val_acc: 0.9889\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ee0e7c9400>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_acc', min_delta=0, patience=0),\n",
    "             ModelCheckpoint(filepath='SavedModel{epoch:02d}.hdf5', monitor='val_acc', save_weights_only=True, period=1)]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32,verbose=2,callbacks=callbacks,shuffle=True, \n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/100\n",
      "55s - loss: 0.0109 - acc: 0.9970 - val_loss: 0.0743 - val_acc: 0.9881\n",
      "Epoch 2/100\n",
      "54s - loss: 0.0094 - acc: 0.9973 - val_loss: 0.0590 - val_acc: 0.9899\n",
      "Epoch 3/100\n",
      "55s - loss: 0.0075 - acc: 0.9983 - val_loss: 0.0685 - val_acc: 0.9894\n",
      "Epoch 4/100\n",
      "54s - loss: 0.0063 - acc: 0.9985 - val_loss: 0.0781 - val_acc: 0.9900\n",
      "Epoch 5/100\n",
      "54s - loss: 0.0055 - acc: 0.9987 - val_loss: 0.0989 - val_acc: 0.9892\n",
      "Epoch 6/100\n",
      "53s - loss: 0.0052 - acc: 0.9986 - val_loss: 0.0888 - val_acc: 0.9900\n",
      "Epoch 7/100\n",
      "55s - loss: 0.0045 - acc: 0.9989 - val_loss: 0.0725 - val_acc: 0.9901\n",
      "Epoch 8/100\n",
      "55s - loss: 0.0050 - acc: 0.9991 - val_loss: 0.0924 - val_acc: 0.9895\n",
      "Epoch 9/100\n",
      "56s - loss: 0.0050 - acc: 0.9991 - val_loss: 0.0874 - val_acc: 0.9890\n",
      "Epoch 10/100\n",
      "56s - loss: 0.0037 - acc: 0.9992 - val_loss: 0.0982 - val_acc: 0.9901\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-d32d02537fdb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m model.fit(X_train, y_train, epochs=100, batch_size=32,verbose=2,callbacks=callbacks,shuffle=True, \n\u001b[1;32m----> 8\u001b[1;33m           validation_data=(X_test, y_test))\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1079\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1080\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2268\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_acc', min_delta=0, patience=3),\n",
    "             ModelCheckpoint(filepath='SavedModel{epoch:02d}.hdf5', monitor='val_acc', save_weights_only=True, period=1)]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32,verbose=2,callbacks=callbacks,shuffle=True, \n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "# I interupted it. My 'patience=3' was too large.\n",
    "\n",
    "# I didn't get a better result than before even though it's the same model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SavedModel00.hdf5\n",
      "SavedModel01.hdf5\n",
      "SavedModel02.hdf5\n",
      "SavedModel03.hdf5\n",
      "SavedModel04.hdf5\n",
      "SavedModel05.hdf5\n"
     ]
    }
   ],
   "source": [
    "# Epoch starts at 0 even though they write 1/100.\n",
    "\n",
    "for epoch in range(6):\n",
    "    filename = 'SavedModel0'+str(epoch)+'.hdf5'\n",
    "    print(filename)\n",
    "    model.load_weights(filename)\n",
    "    ans = argmax(model.predict(xtest), axis=-1)\n",
    "    ans_s = pd.Series(data=ans,index=range(1,28001)).rename('Label')\n",
    "    filename = 'Result'+str(epoch)\n",
    "    ans_s.to_csv(path=filename,header=True,index_label='ImageId')\n",
    "    \n",
    "    \n",
    "# Result0: 0.98814  333 mistakes  It's better than my previous result, but it's just luck since it's still only one epoch.\n",
    "# I got a very smilar result as my CV result 0.9881\n",
    "# Result3: 0.98686  369 mistakes   It's less good than the CV result 0.9900 which is normal since I picked it because of its\n",
    "# large CV\n",
    "\n",
    "# Both are less than my best val_acc is 0.9912, but I didn't check the real result for this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    epoch = 6\n",
    "    filename = 'SavedModel0'+str(epoch)+'.hdf5'\n",
    "    model.load_weights(filename)\n",
    "    ans = argmax(model.predict(xtest), axis=-1)\n",
    "    ans_s = pd.Series(data=ans,index=range(1,28001)).rename('Label')\n",
    "    filename = 'Result'+str(epoch)\n",
    "    ans_s.to_csv(path=filename,header=True,index_label='ImageId')\n",
    "# Result6: 0.98914   305 mistakes   My CV result was 0.9901"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# It is clear that my model overfit quickly since the training accuracy is nearly 1 and the validation accuracy doesn't increase\n",
    "# much if at all after the first epoch which takes less than 1 minute.\n",
    "# It is thus a good idea to have a regularizator such as dropout or weight decay.\n",
    "# Dropout is bad for small network. \n",
    "# It is important that the probabily that the hidden layer be smaller than the output be very small.\n",
    "\n",
    "# A good way to get good results is to try to vary:\n",
    "# Learning Rate\n",
    "# Momentum\n",
    "# Dropout P robability\n",
    "# Weight Decay Rate\n",
    "\n",
    "# I need to see if I want to change relu to leaky relu, parametric relu or random relu, but it's not simple and not everybody\n",
    "# agree on which one is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# One can write model architecture as:\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(100, 100, 3)))\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "\n",
    "# One can add dropout after MaxPool\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "from keras.layers.core import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "# 2D convolution filters, where each filter is of size 5 x 5\n",
    "# 28 x 28 inputs with a single channel for depth \n",
    "# padding=\"same\" means that padding is done such that the output as the same size as the input.\n",
    "model.add(Conv2D(filters=40, kernel_size=5, padding=\"same\", input_shape=(28, 28, 1), activation='relu'))\n",
    "# 2 x 2 max-pooling moving by step of 2 in both directions. Reduce shape to 14 x 14\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "# Dropout\n",
    "model.add(Dropout(0.5))\n",
    "# 50 convolution filters, where each filter is of size 5 x 5\n",
    "model.add(Conv2D(filters=100, kernel_size=5, padding=\"same\", activation='relu'))\n",
    "# Reduce shape to 7 x 7 with 50 channels\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "# Dropout\n",
    "model.add(Dropout(0.5))\n",
    "# Fully connected layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1000, activation='relu'))\n",
    "# Dropout\n",
    "model.add(Dropout(0.5))\n",
    "# softmax classifier  For 10 results\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "          \n",
    "# Maybe change optimizer\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/100\n",
      "176s - loss: 0.1445 - acc: 0.9562 - val_loss: 0.0572 - val_acc: 0.9802\n",
      "Epoch 2/100\n",
      "173s - loss: 0.0940 - acc: 0.9722 - val_loss: 0.0443 - val_acc: 0.9863\n",
      "Epoch 3/100\n",
      "170s - loss: 0.0878 - acc: 0.9747 - val_loss: 0.0406 - val_acc: 0.9869\n",
      "Epoch 4/100\n",
      "7132s - loss: 0.0885 - acc: 0.9766 - val_loss: 0.1175 - val_acc: 0.9804\n",
      "Epoch 5/100\n",
      "175s - loss: 0.0958 - acc: 0.9749 - val_loss: 0.1530 - val_acc: 0.9844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ee0f9a4160>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [EarlyStopping(monitor='val_acc', min_delta=0.002, patience=2),\n",
    "             ModelCheckpoint(filepath='SavedModelDropout{epoch:02d}.hdf5', monitor='val_acc', save_weights_only=True, period=1)]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32,verbose=2,callbacks=callbacks,shuffle=True, \n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SavedModelDropout00.hdf5\n",
      "SavedModelDropout01.hdf5\n",
      "SavedModelDropout02.hdf5\n",
      "SavedModelDropout03.hdf5\n",
      "SavedModelDropout04.hdf5\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    filename = 'SavedModelDropout0'+str(epoch)+'.hdf5'\n",
    "    print(filename)\n",
    "    model.load_weights(filename)\n",
    "    ans = argmax(model.predict(xtest), axis=-1)\n",
    "    ans_s = pd.Series(data=ans,index=range(1,28001)).rename('Label')\n",
    "    filename = 'ResultDropout'+str(epoch)\n",
    "    ans_s.to_csv(path=filename,header=True,index_label='ImageId')\n",
    "    \n",
    "    \n",
    "# ResultDropout0: 0.98143  520 mistakes, very bad but it's normal since learning is slower with dropout. It's also close of my CV\n",
    "# ResultDropout2: 0.98614  388 mistakes  which is not better than my best result, but it's not tha surprising since the CV wasn't\n",
    "# better.\n",
    "\n",
    "# The main reason why it didn't improve are probably:\n",
    "# I should have kept patience=3 or even higher. In my previous result Result6 was obtained after 2 bad result.\n",
    "# The NN was underfitting. In fact, training accuracy was lower than validation accuracy, which is not supposed to happen.\n",
    "# It must be because dropout is not used for validating and the use of the full network is better than one with dropout.\n",
    "# I should use more training time with a bigger NN, so that my network be able to fit the data.\n",
    "\n",
    "# Also, I read that with dropout one must use a bigger learning rate than without it. I should look at how the learning rate is\n",
    "# determined\n",
    "# However, because of the higher learnig rate, a lot of Relu units die. It's better to replace them with ELU or Maxout.\n",
    "# LRelu, PRelu and RRelu that have been created to solve the same problem since to be less good.\n",
    "# Maxout seems to have the best performance, but it's slower.\n",
    "\n",
    "# rmsprop is a creation of G. Hinton which seems to be better than momentum. It's a variation of Stochastic Gradient Descent\n",
    "# in which the learning rate change, like momentum is.\n",
    "\n",
    "# Choice that one must test on:\n",
    "# non-linearity (ReLU, ELU, maxout, compatability with batch normalization)\n",
    "# pooling variants (stochastic, max, average, mixed)\n",
    "# network width\n",
    "# classifier design (convolutional, fully-connected, SPP)\n",
    "# image pre-processing\n",
    "# learning parameters: learning rate, batch size, cleanliness of the data\n",
    "# Should I use Batch normalization? If yes, before or after Relu.\n",
    "#    Batch normalization potentially helps in two ways: faster learning and higher overall accuracy.\n",
    "#    It is said that Batch normalisation washes out differences between ReLU-family variants, so we should use Relu.\n",
    "#    It's seems it is better to use it than using ELU, and it should usually be after Relu.\n",
    "\n",
    "# In this case: https://arxiv.org/pdf/1606.02228v2.pdf\n",
    "# Which is however for a neuronet that has been trained a very long time on a more complex problem.\n",
    "# Maxout is best, but ELU at the beggining and Maxout at the end is a good compromise to reduce computational coat.\n",
    "# Mixing of Max pooling and Average pooling is the best.\n",
    "# Linear decay of learning rate is the best learning rate policy. lr = L0(1 − i/M) where M is the number of learning iterations\n",
    "# Final recommendations:\n",
    "# use ELU non-linearity without batchnorm or ReLU with it.\n",
    "# use the linear learning rate decay policy.\n",
    "# use a sum of the average and max pooling layers.\n",
    "# use mini-batch size around 128 or 256. If this is too big for your GPU,\n",
    "#   decrease the learning rate proportionally to the batch size.\n",
    "# use fully-connected layers as convolutional and average the predictions for the final decision\n",
    "\n",
    "# It is often the case that the model stops improving and then jump to a better result. So one should have a large patience.\n",
    "\n",
    "# For batch normalization, put it after Relu\n",
    "# from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nfilters1: 40, Nfilters2: 100, kernelsize1: 3, kernelsize2: 3, learningRate: 0.001, dropout: 0.2\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/100\n",
      "219s - loss: 0.2752 - acc: 0.9201 - val_loss: 0.1056 - val_acc: 0.9744\n",
      "Epoch 2/100\n",
      "215s - loss: 0.1778 - acc: 0.9526 - val_loss: 0.1054 - val_acc: 0.9735\n",
      "Epoch 3/100\n",
      "214s - loss: 0.1665 - acc: 0.9575 - val_loss: 0.0852 - val_acc: 0.9787\n",
      "Epoch 4/100\n",
      "209s - loss: 0.1596 - acc: 0.9607 - val_loss: 0.1011 - val_acc: 0.9751\n",
      "Epoch 5/100\n",
      "212s - loss: 0.1603 - acc: 0.9638 - val_loss: 0.0885 - val_acc: 0.9799\n",
      "Epoch 6/100\n",
      "209s - loss: 0.1590 - acc: 0.9658 - val_loss: 0.1057 - val_acc: 0.9765\n",
      "Epoch 7/100\n",
      "212s - loss: 0.1503 - acc: 0.9677 - val_loss: 0.0880 - val_acc: 0.9821\n",
      "Epoch 8/100\n",
      "209s - loss: 0.1589 - acc: 0.9685 - val_loss: 0.1375 - val_acc: 0.9719\n",
      "Epoch 9/100\n",
      "218s - loss: 0.1644 - acc: 0.9704 - val_loss: 0.0754 - val_acc: 0.9850\n",
      "Epoch 10/100\n",
      "205s - loss: 0.1438 - acc: 0.9748 - val_loss: 0.1192 - val_acc: 0.9819\n",
      "Epoch 11/100\n",
      "3737s - loss: 0.1501 - acc: 0.9738 - val_loss: 0.0881 - val_acc: 0.9865\n",
      "Epoch 12/100\n",
      "3038s - loss: 0.1496 - acc: 0.9765 - val_loss: 0.1170 - val_acc: 0.9830\n",
      "Epoch 13/100\n",
      "217s - loss: 0.1529 - acc: 0.9779 - val_loss: 0.0993 - val_acc: 0.9854\n",
      "Epoch 14/100\n",
      "215s - loss: 0.1565 - acc: 0.9782 - val_loss: 0.1153 - val_acc: 0.9843\n",
      "Epoch 15/100\n",
      "219s - loss: 0.1414 - acc: 0.9798 - val_loss: 0.1176 - val_acc: 0.9850\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "Nfilters1: 40, Nfilters2: 100, kernelsize1: 3, kernelsize2: 3, learningRate: 0.001, dropout: 0.5\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/100\n",
      "223s - loss: 0.2688 - acc: 0.9214 - val_loss: 0.1072 - val_acc: 0.9702\n",
      "Epoch 2/100\n",
      "222s - loss: 0.1777 - acc: 0.9538 - val_loss: 0.0933 - val_acc: 0.9774\n",
      "Epoch 3/100\n",
      "224s - loss: 0.1688 - acc: 0.9569 - val_loss: 0.0856 - val_acc: 0.9785\n",
      "Epoch 4/100\n",
      "224s - loss: 0.1572 - acc: 0.9614 - val_loss: 0.0905 - val_acc: 0.9780\n",
      "Epoch 5/100\n",
      "227s - loss: 0.1575 - acc: 0.9629 - val_loss: 0.0893 - val_acc: 0.9824\n",
      "Epoch 6/100\n",
      "227s - loss: 0.1606 - acc: 0.9650 - val_loss: 0.0750 - val_acc: 0.9845\n",
      "Epoch 7/100\n",
      "227s - loss: 0.1558 - acc: 0.9677 - val_loss: 0.0855 - val_acc: 0.9812\n",
      "Epoch 8/100\n",
      "228s - loss: 0.1465 - acc: 0.9713 - val_loss: 0.0903 - val_acc: 0.9826\n",
      "Epoch 9/100\n",
      "229s - loss: 0.1552 - acc: 0.9707 - val_loss: 0.0836 - val_acc: 0.9838\n",
      "Epoch 10/100\n",
      "229s - loss: 0.1473 - acc: 0.9741 - val_loss: 0.0957 - val_acc: 0.9849\n",
      "Epoch 11/100\n",
      "226s - loss: 0.1483 - acc: 0.9763 - val_loss: 0.1045 - val_acc: 0.9844\n",
      "Epoch 12/100\n",
      "224s - loss: 0.1463 - acc: 0.9776 - val_loss: 0.1254 - val_acc: 0.9812\n",
      "Epoch 13/100\n",
      "224s - loss: 0.1514 - acc: 0.9780 - val_loss: 0.0960 - val_acc: 0.9860\n",
      "Epoch 14/100\n",
      "223s - loss: 0.1528 - acc: 0.9792 - val_loss: 0.1231 - val_acc: 0.9840\n",
      "Epoch 15/100\n",
      "222s - loss: 0.1523 - acc: 0.9799 - val_loss: 0.0989 - val_acc: 0.9874\n",
      "Epoch 16/100\n",
      "221s - loss: 0.1466 - acc: 0.9810 - val_loss: 0.0993 - val_acc: 0.9869\n",
      "Epoch 17/100\n",
      "220s - loss: 0.1487 - acc: 0.9812 - val_loss: 0.0909 - val_acc: 0.9887\n",
      "Epoch 18/100\n",
      "219s - loss: 0.1473 - acc: 0.9820 - val_loss: 0.1177 - val_acc: 0.9865\n",
      "Epoch 19/100\n",
      "219s - loss: 0.1442 - acc: 0.9839 - val_loss: 0.1303 - val_acc: 0.9882\n",
      "Epoch 20/100\n",
      "218s - loss: 0.1654 - acc: 0.9825 - val_loss: 0.1142 - val_acc: 0.9895\n",
      "Epoch 21/100\n",
      "218s - loss: 0.1511 - acc: 0.9844 - val_loss: 0.1098 - val_acc: 0.9888\n",
      "Epoch 22/100\n",
      "218s - loss: 0.1629 - acc: 0.9840 - val_loss: 0.1074 - val_acc: 0.9900\n",
      "Epoch 23/100\n",
      "217s - loss: 0.1607 - acc: 0.9844 - val_loss: 0.1154 - val_acc: 0.9892\n",
      "Epoch 24/100\n",
      "214s - loss: 0.1553 - acc: 0.9852 - val_loss: 0.1302 - val_acc: 0.9892\n",
      "Epoch 25/100\n",
      "212s - loss: 0.1730 - acc: 0.9846 - val_loss: 0.1431 - val_acc: 0.9879\n",
      "Epoch 26/100\n",
      "208s - loss: 0.1494 - acc: 0.9865 - val_loss: 0.1388 - val_acc: 0.9885\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "Nfilters1: 40, Nfilters2: 100, kernelsize1: 3, kernelsize2: 3, learningRate: 0.01, dropout: 0.2\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/100\n",
      "225s - loss: 14.4994 - acc: 0.0997 - val_loss: 14.6118 - val_acc: 0.0935\n",
      "Epoch 2/100\n",
      "225s - loss: 14.5101 - acc: 0.0998 - val_loss: 14.6118 - val_acc: 0.0935\n",
      "Epoch 3/100\n",
      "236s - loss: 14.5101 - acc: 0.0998 - val_loss: 14.6118 - val_acc: 0.0935\n",
      "Epoch 4/100\n",
      "231s - loss: 14.5101 - acc: 0.0998 - val_loss: 14.6118 - val_acc: 0.0935\n",
      "Epoch 5/100\n",
      "229s - loss: 14.5101 - acc: 0.0998 - val_loss: 14.6118 - val_acc: 0.0935\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "Nfilters1: 40, Nfilters2: 100, kernelsize1: 3, kernelsize2: 3, learningRate: 0.01, dropout: 0.5\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/100\n",
      "229s - loss: 14.5159 - acc: 0.0987 - val_loss: 14.5513 - val_acc: 0.0971\n",
      "Epoch 2/100\n",
      "223s - loss: 14.5274 - acc: 0.0987 - val_loss: 14.5513 - val_acc: 0.0971\n",
      "Epoch 3/100\n",
      "223s - loss: 14.5274 - acc: 0.0987 - val_loss: 14.5513 - val_acc: 0.0971\n",
      "Epoch 4/100\n",
      "225s - loss: 14.5274 - acc: 0.0987 - val_loss: 14.5513 - val_acc: 0.0971\n",
      "Epoch 5/100\n",
      "232s - loss: 14.5274 - acc: 0.0987 - val_loss: 14.5513 - val_acc: 0.0971\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "Nfilters1: 40, Nfilters2: 100, kernelsize1: 3, kernelsize2: 3, learningRate: 0.1, dropout: 0.2\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/100\n",
      "244s - loss: 14.4759 - acc: 0.1012 - val_loss: 14.3202 - val_acc: 0.1115\n",
      "Epoch 2/100\n",
      "234s - loss: 14.4804 - acc: 0.1016 - val_loss: 14.3202 - val_acc: 0.1115\n",
      "Epoch 3/100\n",
      "230s - loss: 14.4804 - acc: 0.1016 - val_loss: 14.3202 - val_acc: 0.1115\n",
      "Epoch 4/100\n",
      "227s - loss: 14.4804 - acc: 0.1016 - val_loss: 14.3202 - val_acc: 0.1115\n",
      "Epoch 5/100\n",
      "226s - loss: 14.4804 - acc: 0.1016 - val_loss: 14.3202 - val_acc: 0.1115\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "Nfilters1: 40, Nfilters2: 100, kernelsize1: 3, kernelsize2: 3, learningRate: 0.1, dropout: 0.5\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/100\n",
      "226s - loss: 14.5575 - acc: 0.0960 - val_loss: 14.5082 - val_acc: 0.0999\n",
      "Epoch 2/100\n",
      "39588s - loss: 14.5672 - acc: 0.0962 - val_loss: 14.5082 - val_acc: 0.0999\n",
      "Epoch 3/100\n",
      "218s - loss: 14.5672 - acc: 0.0962 - val_loss: 14.5082 - val_acc: 0.0999\n",
      "Epoch 4/100\n",
      "214s - loss: 14.5672 - acc: 0.0962 - val_loss: 14.5082 - val_acc: 0.0999\n",
      "Epoch 5/100\n",
      "1843s - loss: 14.5672 - acc: 0.0962 - val_loss: 14.5082 - val_acc: 0.0999\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "Nfilters1: 40, Nfilters2: 100, kernelsize1: 3, kernelsize2: 5, learningRate: 0.001, dropout: 0.2\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/100\n",
      "255s - loss: 0.2896 - acc: 0.9220 - val_loss: 0.1078 - val_acc: 0.9706\n",
      "Epoch 2/100\n",
      "254s - loss: 0.1877 - acc: 0.9531 - val_loss: 0.0895 - val_acc: 0.9769\n",
      "Epoch 3/100\n",
      "260s - loss: 0.1759 - acc: 0.9581 - val_loss: 0.0985 - val_acc: 0.9795\n",
      "Epoch 4/100\n",
      "255s - loss: 0.1727 - acc: 0.9621 - val_loss: 0.1064 - val_acc: 0.9785\n",
      "Epoch 5/100\n",
      "255s - loss: 0.1701 - acc: 0.9670 - val_loss: 0.0967 - val_acc: 0.9823\n",
      "Epoch 6/100\n",
      "256s - loss: 0.1700 - acc: 0.9698 - val_loss: 0.0979 - val_acc: 0.9815\n",
      "Epoch 7/100\n",
      "266s - loss: 0.1789 - acc: 0.9702 - val_loss: 0.0926 - val_acc: 0.9846\n",
      "Epoch 8/100\n",
      "257s - loss: 0.1635 - acc: 0.9738 - val_loss: 0.1350 - val_acc: 0.9792\n",
      "Epoch 9/100\n",
      "261s - loss: 0.1538 - acc: 0.9765 - val_loss: 0.1005 - val_acc: 0.9856\n",
      "Epoch 10/100\n",
      "262s - loss: 0.1622 - acc: 0.9775 - val_loss: 0.1348 - val_acc: 0.9826\n",
      "Epoch 11/100\n",
      "259s - loss: 0.1713 - acc: 0.9773 - val_loss: 0.1138 - val_acc: 0.9860\n",
      "Epoch 12/100\n",
      "259s - loss: 0.1601 - acc: 0.9806 - val_loss: 0.1151 - val_acc: 0.9858\n",
      "Epoch 13/100\n",
      "254s - loss: 0.1762 - acc: 0.9802 - val_loss: 0.1131 - val_acc: 0.9875\n",
      "Epoch 14/100\n",
      "254s - loss: 0.1742 - acc: 0.9817 - val_loss: 0.1168 - val_acc: 0.9875\n",
      "Epoch 15/100\n",
      "251s - loss: 0.1746 - acc: 0.9821 - val_loss: 0.1480 - val_acc: 0.9865\n",
      "Epoch 16/100\n",
      "250s - loss: 0.1795 - acc: 0.9823 - val_loss: 0.1545 - val_acc: 0.9854\n",
      "Epoch 17/100\n",
      "251s - loss: 0.1725 - acc: 0.9841 - val_loss: 0.1457 - val_acc: 0.9857\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "Nfilters1: 40, Nfilters2: 100, kernelsize1: 3, kernelsize2: 5, learningRate: 0.001, dropout: 0.5\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/100\n",
      "250s - loss: 0.2914 - acc: 0.9198 - val_loss: 0.1040 - val_acc: 0.9737\n",
      "Epoch 2/100\n",
      "254s - loss: 0.1849 - acc: 0.9542 - val_loss: 0.0888 - val_acc: 0.9770\n",
      "Epoch 3/100\n",
      "264s - loss: 0.1776 - acc: 0.9595 - val_loss: 0.1087 - val_acc: 0.9733\n",
      "Epoch 4/100\n",
      "260s - loss: 0.1712 - acc: 0.9633 - val_loss: 0.1039 - val_acc: 0.9794\n",
      "Epoch 5/100\n",
      "256s - loss: 0.1687 - acc: 0.9664 - val_loss: 0.1001 - val_acc: 0.9805\n",
      "Epoch 6/100\n",
      "253s - loss: 0.1654 - acc: 0.9679 - val_loss: 0.1137 - val_acc: 0.9811\n",
      "Epoch 7/100\n",
      "257s - loss: 0.1657 - acc: 0.9718 - val_loss: 0.1230 - val_acc: 0.9817\n",
      "Epoch 8/100\n",
      "256s - loss: 0.1660 - acc: 0.9734 - val_loss: 0.0939 - val_acc: 0.9860\n",
      "Epoch 9/100\n",
      "258s - loss: 0.1591 - acc: 0.9769 - val_loss: 0.1080 - val_acc: 0.9870\n",
      "Epoch 10/100\n",
      "252s - loss: 0.1663 - acc: 0.9768 - val_loss: 0.0976 - val_acc: 0.9879\n",
      "Epoch 11/100\n",
      "255s - loss: 0.1616 - acc: 0.9790 - val_loss: 0.1541 - val_acc: 0.9821\n",
      "Epoch 12/100\n",
      "254s - loss: 0.1571 - acc: 0.9819 - val_loss: 0.1480 - val_acc: 0.9851\n",
      "Epoch 13/100\n",
      "257s - loss: 0.1728 - acc: 0.9806 - val_loss: 0.1274 - val_acc: 0.9865\n",
      "Epoch 14/100\n",
      "256s - loss: 0.1686 - acc: 0.9820 - val_loss: 0.1571 - val_acc: 0.9856\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "Nfilters1: 40, Nfilters2: 100, kernelsize1: 3, kernelsize2: 5, learningRate: 0.01, dropout: 0.2\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/100\n",
      "255s - loss: 14.5055 - acc: 0.0995 - val_loss: 14.4948 - val_acc: 0.1007\n",
      "Epoch 2/100\n",
      "256s - loss: 14.5202 - acc: 0.0991 - val_loss: 14.4948 - val_acc: 0.1007\n",
      "Epoch 3/100\n",
      "255s - loss: 14.5202 - acc: 0.0991 - val_loss: 14.4948 - val_acc: 0.1007\n",
      "Epoch 4/100\n",
      "253s - loss: 14.5202 - acc: 0.0991 - val_loss: 14.4948 - val_acc: 0.1007\n",
      "Epoch 5/100\n",
      "262s - loss: 14.5202 - acc: 0.0991 - val_loss: 14.4948 - val_acc: 0.1007\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "Nfilters1: 40, Nfilters2: 100, kernelsize1: 3, kernelsize2: 5, learningRate: 0.01, dropout: 0.5\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/100\n",
      "261s - loss: 14.4216 - acc: 0.1044 - val_loss: 14.4046 - val_acc: 0.1063\n",
      "Epoch 2/100\n",
      "253s - loss: 14.4353 - acc: 0.1044 - val_loss: 14.4046 - val_acc: 0.1063\n",
      "Epoch 3/100\n",
      "254s - loss: 14.4353 - acc: 0.1044 - val_loss: 14.4046 - val_acc: 0.1063\n",
      "Epoch 4/100\n",
      "256s - loss: 14.4353 - acc: 0.1044 - val_loss: 14.4046 - val_acc: 0.1063\n",
      "Epoch 5/100\n",
      "234s - loss: 14.4353 - acc: 0.1044 - val_loss: 14.4046 - val_acc: 0.1063\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "Nfilters1: 40, Nfilters2: 100, kernelsize1: 3, kernelsize2: 5, learningRate: 0.1, dropout: 0.2\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/100\n",
      "238s - loss: 14.2938 - acc: 0.1124 - val_loss: 14.3739 - val_acc: 0.1082\n",
      "Epoch 2/100\n",
      "235s - loss: 14.3072 - acc: 0.1124 - val_loss: 14.3739 - val_acc: 0.1082\n",
      "Epoch 3/100\n",
      "234s - loss: 14.3072 - acc: 0.1124 - val_loss: 14.3739 - val_acc: 0.1082\n",
      "Epoch 4/100\n",
      "234s - loss: 14.3072 - acc: 0.1124 - val_loss: 14.3739 - val_acc: 0.1082\n",
      "Epoch 5/100\n",
      "250s - loss: 14.3072 - acc: 0.1124 - val_loss: 14.3739 - val_acc: 0.1082\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "Nfilters1: 40, Nfilters2: 100, kernelsize1: 3, kernelsize2: 5, learningRate: 0.1, dropout: 0.5\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/100\n",
      "260s - loss: 14.5590 - acc: 0.0960 - val_loss: 14.5082 - val_acc: 0.0999\n",
      "Epoch 2/100\n",
      "248s - loss: 14.5672 - acc: 0.0962 - val_loss: 14.5082 - val_acc: 0.0999\n",
      "Epoch 3/100\n",
      "236s - loss: 14.5672 - acc: 0.0962 - val_loss: 14.5082 - val_acc: 0.0999\n",
      "Epoch 4/100\n",
      "244s - loss: 14.5672 - acc: 0.0962 - val_loss: 14.5082 - val_acc: 0.0999\n",
      "Epoch 5/100\n",
      "250s - loss: 14.5672 - acc: 0.0962 - val_loss: 14.5082 - val_acc: 0.0999\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "Nfilters1: 40, Nfilters2: 100, kernelsize1: 5, kernelsize2: 3, learningRate: 0.001, dropout: 0.2\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/100\n",
      "224s - loss: 0.2872 - acc: 0.9169 - val_loss: 0.1236 - val_acc: 0.9681\n",
      "Epoch 2/100\n",
      "221s - loss: 0.1985 - acc: 0.9493 - val_loss: 0.1142 - val_acc: 0.9689\n",
      "Epoch 3/100\n",
      "223s - loss: 0.1923 - acc: 0.9545 - val_loss: 0.0962 - val_acc: 0.9808\n",
      "Epoch 4/100\n",
      "222s - loss: 0.1902 - acc: 0.9608 - val_loss: 0.1501 - val_acc: 0.9662\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-cab0ffccb047>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m                         model.fit(X_train, y_train, epochs=100, batch_size=32,verbose=2,callbacks=callbacks,shuffle=True, \n\u001b[1;32m---> 47\u001b[1;33m                                   validation_data=(X_test, y_test))\n\u001b[0m\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_______________________________________________________________________________\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1079\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1080\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2268\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# elu\n",
    "# patience = 3\n",
    "# Wider network\n",
    "\n",
    "# I should maybe make a grid that select the best parameter to get the best val-acc\n",
    "# I will use save_best_only=True for ModelCheckPoint to get only the best model.\n",
    "\n",
    "# I can change learning rate of rmsprop. It is recommended not to change the other parameters.\n",
    "# keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "for Nfilters1 in [40, 80]:\n",
    "    for Nfilters2 in [100, 200]:\n",
    "        for kernelsize1 in [3, 5]:\n",
    "            for kernelsize2 in [3, 5]:\n",
    "                for learningRate in [0.001, 0.01, 0.1]:\n",
    "                    for dropout in [0.2, 0.5]:\n",
    "                        model = Sequential()\n",
    "                        model.add(Conv2D(filters=Nfilters1, kernel_size=kernelsize1, padding=\"same\", input_shape=(28, 28, 1), activation='elu'))\n",
    "                        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "                        model.add(Dropout(0.5))\n",
    "                        model.add(Conv2D(filters=Nfilters2, kernel_size=kernelsize2, padding=\"same\", activation='elu'))\n",
    "                        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "                        model.add(Dropout(0.5))\n",
    "                        model.add(Flatten())\n",
    "                        model.add(Dense(2000, activation='elu'))\n",
    "                        model.add(Dropout(0.5))\n",
    "                        model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "                        model.compile(optimizer=RMSprop(lr=learningRate),\n",
    "                                      loss='categorical_crossentropy',\n",
    "                                      metrics=['accuracy'])\n",
    "                        \n",
    "                        filename = (\"SavedModel_\" + str(Nfilters1) + \"_\" + str(Nfilters2) + \"_\" + str(kernelsize1) + \"_\" + \n",
    "                                    str(kernelsize2) + \"_\" + str(learningRate) + \"_\" + str(dropout) + \".hdf5\")\n",
    "\n",
    "                        callbacks = [EarlyStopping(monitor='val_acc', min_delta=0., patience=3),\n",
    "                                     ModelCheckpoint(filepath=filename, monitor='val_acc', save_best_only=True, save_weights_only=True, \n",
    "                                                     period=1)]\n",
    "                        \n",
    "                        print(\"Nfilters1: \"+ str(Nfilters1) +\", Nfilters2: \"+ str(Nfilters2) +\", kernelsize1: \"+ \n",
    "                              str(kernelsize1) +\", \"+ \"kernelsize2: \"+ str(kernelsize2) +\", learningRate: \"+ str(learningRate) +\n",
    "                              \", dropout: \"+ str(dropout))\n",
    "\n",
    "                        model.fit(X_train, y_train, epochs=100, batch_size=32,verbose=2,callbacks=callbacks,shuffle=True, \n",
    "                                  validation_data=(X_test, y_test))\n",
    "                        \n",
    "                        print(\"_______________________________________________________________________________\")\n",
    "                        print(\"_______________________________________________________________________________\")\n",
    "                        print(\"_______________________________________________________________________________\")\n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I forgot to vary the number of units in the dense layer. I should do that next.\n",
    "# I forgot to vary batch_size, and I read it's recommended to use 128 or 256.\n",
    "# I made a mistake, dropout is always 0.5\n",
    "\n",
    "# Big time are due to my computer going to sleep.\n",
    "\n",
    "# Conclusion of the variation:\n",
    "# Learning rate of 0.01 and 0.1 are too big. They are a waste of time.\n",
    "\n",
    "# Nfilters1: 40, Nfilters2: 100, kernelsize1: 3, kernelsize2: 3, learningRate: 0.001, dropout: 0.5\n",
    "# 0.9900   225s by epoch, 22 epochs\n",
    "# The same thing got me:    0.9854   210s by epoch, 13 epochs     \n",
    "# but it might be because patience is too low since it's still underfitting.\n",
    "# It beats kernelsize2 = 5   0.9875   250s by epoch, 14 epochs\n",
    "# It beats again kernelsize2 = 5   0.9879   250s by epoch, 10 epochs\n",
    "\n",
    "\n",
    "# What I read about gradient descent:\n",
    "# Momentum is good to make the parameters evolve well when it's a ravine. Nesterov accelerated gradient is an modified momentum.\n",
    "# Momentum: Compute the gradient, then add a fraction of previous step.\n",
    "# Nesterov accelerated: Add fraction of previoud step before computing the gradient.\n",
    "# However, it seems that people are prefer momentum over Nesterov now.\n",
    "\n",
    "# Adadelta is similar to momentum, but the learning rate is replaced by the root mean squared error of parameter updates\n",
    "# so we don't need a learning rate.\n",
    "# RMSprop is similar but need a learning rate that the author say should be 0.001 by default.\n",
    "# Both Adadelta and RMSprop have been developped at the same time independantly.\n",
    "\n",
    "# Adam is a combination of Momentum and RMSprop, while NAdam is a combination of Nesterov and RMSprop.\n",
    "\n",
    "# I read that: \"Adam might be the best overall choice.\"\n",
    "\n",
    "# Advice about gradient descent:\n",
    "# Shuffle data after each epoch. (Unless we want to have them by increasing difficulty which is not my case.)\n",
    "# One should thus always monitor error on a validation set during training and stop (with some patience) if the validation error does not improve enough.\n",
    "# Batch normalization is good. It allows higher learning rate and reduce the need for dropout.\n",
    "\n",
    "# Adam is considered as the state of art optimizer.\n",
    "# Other say that SGD with momentum is better.\n",
    "# It seems that for sparse data Adam is better, for non-sparse data SDG with momentum can be better, but it needs to tune the\n",
    "# learning rate.\n",
    "\n",
    "\n",
    "\n",
    "# I read:\n",
    "# The patience is often set somewhere between 10 and 100 (10 or 20 is more common)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nfilters1: 40, Nfilters2: 100, kernelsize1: 3, kernelsize2: 3, Nunits: 1000, dropout: 0.0, batchsize: 128\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/1000\n",
      "224s - loss: 0.1380 - acc: 0.9598 - val_loss: 0.0680 - val_acc: 0.9788\n",
      "Epoch 2/1000\n",
      "220s - loss: 0.0577 - acc: 0.9816 - val_loss: 0.0715 - val_acc: 0.9767\n",
      "Epoch 3/1000\n",
      "213s - loss: 0.0448 - acc: 0.9857 - val_loss: 0.0647 - val_acc: 0.9804\n",
      "Epoch 4/1000\n",
      "231s - loss: 0.0305 - acc: 0.9900 - val_loss: 0.0673 - val_acc: 0.9796\n",
      "Epoch 5/1000\n",
      "226s - loss: 0.0300 - acc: 0.9906 - val_loss: 0.0600 - val_acc: 0.9824\n",
      "Epoch 6/1000\n",
      "225s - loss: 0.0266 - acc: 0.9915 - val_loss: 0.0626 - val_acc: 0.9840\n",
      "Epoch 7/1000\n",
      "224s - loss: 0.0194 - acc: 0.9936 - val_loss: 0.0317 - val_acc: 0.9917\n",
      "Epoch 8/1000\n",
      "211s - loss: 0.0182 - acc: 0.9944 - val_loss: 0.0423 - val_acc: 0.9885\n",
      "Epoch 9/1000\n",
      "208s - loss: 0.0169 - acc: 0.9947 - val_loss: 0.0349 - val_acc: 0.9906\n",
      "Epoch 10/1000\n",
      "208s - loss: 0.0154 - acc: 0.9948 - val_loss: 0.0435 - val_acc: 0.9885\n",
      "Epoch 11/1000\n",
      "208s - loss: 0.0114 - acc: 0.9962 - val_loss: 0.0558 - val_acc: 0.9879\n",
      "Epoch 12/1000\n",
      "208s - loss: 0.0122 - acc: 0.9963 - val_loss: 0.0566 - val_acc: 0.9874\n",
      "Epoch 13/1000\n",
      "213s - loss: 0.0118 - acc: 0.9961 - val_loss: 0.0608 - val_acc: 0.9858\n",
      "Epoch 14/1000\n",
      "208s - loss: 0.0094 - acc: 0.9973 - val_loss: 0.0413 - val_acc: 0.9900\n",
      "Epoch 15/1000\n",
      "211s - loss: 0.0086 - acc: 0.9975 - val_loss: 0.0872 - val_acc: 0.9837\n",
      "Epoch 16/1000\n",
      "230s - loss: 0.0097 - acc: 0.9975 - val_loss: 0.0517 - val_acc: 0.9885\n",
      "Epoch 17/1000\n",
      "228s - loss: 0.0096 - acc: 0.9968 - val_loss: 0.0473 - val_acc: 0.9893\n",
      "Epoch 18/1000\n",
      "227s - loss: 0.0074 - acc: 0.9979 - val_loss: 0.0496 - val_acc: 0.9892\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "Nfilters1: 40, Nfilters2: 100, kernelsize1: 3, kernelsize2: 3, Nunits: 1000, dropout: 0.0, batchsize: 256\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/1000\n",
      "221s - loss: 0.1299 - acc: 0.9609 - val_loss: 0.0770 - val_acc: 0.9757\n",
      "Epoch 2/1000\n",
      "229s - loss: 0.0556 - acc: 0.9824 - val_loss: 0.0882 - val_acc: 0.9774\n",
      "Epoch 3/1000\n",
      "224s - loss: 0.0390 - acc: 0.9883 - val_loss: 0.0497 - val_acc: 0.9826\n",
      "Epoch 4/1000\n",
      "223s - loss: 0.0329 - acc: 0.9895 - val_loss: 0.0518 - val_acc: 0.9831\n",
      "Epoch 5/1000\n",
      "226s - loss: 0.0267 - acc: 0.9913 - val_loss: 0.0712 - val_acc: 0.9811\n",
      "Epoch 6/1000\n",
      "227s - loss: 0.0217 - acc: 0.9929 - val_loss: 0.0489 - val_acc: 0.9865\n",
      "Epoch 7/1000\n",
      "224s - loss: 0.0223 - acc: 0.9930 - val_loss: 0.0415 - val_acc: 0.9877\n",
      "Epoch 8/1000\n",
      "230s - loss: 0.0141 - acc: 0.9959 - val_loss: 0.0472 - val_acc: 0.9879\n",
      "Epoch 9/1000\n",
      "226s - loss: 0.0149 - acc: 0.9955 - val_loss: 0.0337 - val_acc: 0.9911\n",
      "Epoch 10/1000\n",
      "219s - loss: 0.0138 - acc: 0.9958 - val_loss: 0.0540 - val_acc: 0.9881\n",
      "Epoch 11/1000\n",
      "211s - loss: 0.0118 - acc: 0.9962 - val_loss: 0.0484 - val_acc: 0.9902\n",
      "Epoch 12/1000\n",
      "222s - loss: 0.0112 - acc: 0.9966 - val_loss: 0.0520 - val_acc: 0.9875\n",
      "Epoch 13/1000\n",
      "224s - loss: 0.0096 - acc: 0.9972 - val_loss: 0.0398 - val_acc: 0.9905\n",
      "Epoch 14/1000\n",
      "230s - loss: 0.0061 - acc: 0.9983 - val_loss: 0.0573 - val_acc: 0.9886\n",
      "Epoch 15/1000\n",
      "232s - loss: 0.0129 - acc: 0.9965 - val_loss: 0.0531 - val_acc: 0.9890\n",
      "Epoch 16/1000\n",
      "222s - loss: 0.0082 - acc: 0.9973 - val_loss: 0.0428 - val_acc: 0.9910\n",
      "Epoch 17/1000\n",
      "215s - loss: 0.0076 - acc: 0.9976 - val_loss: 0.0475 - val_acc: 0.9893\n",
      "Epoch 18/1000\n",
      "220s - loss: 0.0082 - acc: 0.9977 - val_loss: 0.0386 - val_acc: 0.9920\n",
      "Epoch 19/1000\n",
      "218s - loss: 0.0061 - acc: 0.9982 - val_loss: 0.0438 - val_acc: 0.9902\n",
      "Epoch 20/1000\n",
      "211s - loss: 0.0061 - acc: 0.9981 - val_loss: 0.0441 - val_acc: 0.9902\n",
      "Epoch 21/1000\n",
      "209s - loss: 0.0041 - acc: 0.9988 - val_loss: 0.0587 - val_acc: 0.9893\n",
      "Epoch 22/1000\n",
      "203s - loss: 0.0079 - acc: 0.9977 - val_loss: 0.0458 - val_acc: 0.9913\n",
      "Epoch 23/1000\n",
      "203s - loss: 0.0045 - acc: 0.9983 - val_loss: 0.0496 - val_acc: 0.9905\n",
      "Epoch 24/1000\n",
      "204s - loss: 0.0046 - acc: 0.9988 - val_loss: 0.0466 - val_acc: 0.9906\n",
      "Epoch 25/1000\n",
      "206s - loss: 0.0033 - acc: 0.9991 - val_loss: 0.0424 - val_acc: 0.9926\n",
      "Epoch 26/1000\n",
      "206s - loss: 0.0060 - acc: 0.9985 - val_loss: 0.0500 - val_acc: 0.9899\n",
      "Epoch 27/1000\n",
      "203s - loss: 0.0066 - acc: 0.9980 - val_loss: 0.0565 - val_acc: 0.9907\n",
      "Epoch 28/1000\n",
      "204s - loss: 0.0048 - acc: 0.9987 - val_loss: 0.0512 - val_acc: 0.9908\n",
      "Epoch 29/1000\n",
      "204s - loss: 0.0049 - acc: 0.9985 - val_loss: 0.0479 - val_acc: 0.9910\n",
      "Epoch 30/1000\n",
      "8073s - loss: 0.0045 - acc: 0.9987 - val_loss: 0.0516 - val_acc: 0.9907\n",
      "Epoch 31/1000\n",
      "209s - loss: 0.0035 - acc: 0.9990 - val_loss: 0.0536 - val_acc: 0.9906\n",
      "Epoch 32/1000\n",
      "218s - loss: 0.0050 - acc: 0.9985 - val_loss: 0.0505 - val_acc: 0.9918\n",
      "Epoch 33/1000\n",
      "665s - loss: 0.0033 - acc: 0.9990 - val_loss: 0.0473 - val_acc: 0.9914\n",
      "Epoch 34/1000\n",
      "225s - loss: 0.0027 - acc: 0.9993 - val_loss: 0.0443 - val_acc: 0.9923\n",
      "Epoch 35/1000\n",
      "218s - loss: 0.0045 - acc: 0.9989 - val_loss: 0.0545 - val_acc: 0.9905\n",
      "Epoch 36/1000\n",
      "214s - loss: 0.0033 - acc: 0.9992 - val_loss: 0.0495 - val_acc: 0.9914\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "_______________________________________________________________________________\n",
      "Nfilters1: 40, Nfilters2: 100, kernelsize1: 3, kernelsize2: 3, Nunits: 1000, dropout: 0.2, batchsize: 128\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/1000\n",
      "220s - loss: 0.1650 - acc: 0.9507 - val_loss: 0.0784 - val_acc: 0.9756\n",
      "Epoch 2/1000\n",
      "217s - loss: 0.0790 - acc: 0.9757 - val_loss: 0.0594 - val_acc: 0.9813\n",
      "Epoch 3/1000\n",
      "229s - loss: 0.0592 - acc: 0.9813 - val_loss: 0.0446 - val_acc: 0.9862\n",
      "Epoch 4/1000\n",
      "229s - loss: 0.0515 - acc: 0.9843 - val_loss: 0.0449 - val_acc: 0.9860\n",
      "Epoch 5/1000\n",
      "226s - loss: 0.0436 - acc: 0.9862 - val_loss: 0.0452 - val_acc: 0.9869\n",
      "Epoch 6/1000\n",
      "224s - loss: 0.0404 - acc: 0.9871 - val_loss: 0.0343 - val_acc: 0.9892\n",
      "Epoch 7/1000\n",
      "226s - loss: 0.0329 - acc: 0.9893 - val_loss: 0.0426 - val_acc: 0.9864\n",
      "Epoch 8/1000\n",
      "229s - loss: 0.0301 - acc: 0.9905 - val_loss: 0.0425 - val_acc: 0.9885\n",
      "Epoch 9/1000\n",
      "218s - loss: 0.0313 - acc: 0.9902 - val_loss: 0.0411 - val_acc: 0.9875\n",
      "Epoch 10/1000\n",
      "220s - loss: 0.0254 - acc: 0.9917 - val_loss: 0.0451 - val_acc: 0.9886\n",
      "Epoch 11/1000\n",
      "213s - loss: 0.0198 - acc: 0.9937 - val_loss: 0.0341 - val_acc: 0.9908\n",
      "Epoch 12/1000\n",
      "215s - loss: 0.0259 - acc: 0.9918 - val_loss: 0.0469 - val_acc: 0.9882\n",
      "Epoch 13/1000\n",
      "227s - loss: 0.0187 - acc: 0.9935 - val_loss: 0.0414 - val_acc: 0.9900\n",
      "Epoch 14/1000\n",
      "228s - loss: 0.0178 - acc: 0.9940 - val_loss: 0.0454 - val_acc: 0.9896\n",
      "Epoch 15/1000\n",
      "230s - loss: 0.0173 - acc: 0.9947 - val_loss: 0.0351 - val_acc: 0.9918\n",
      "Epoch 16/1000\n",
      "218s - loss: 0.0162 - acc: 0.9951 - val_loss: 0.0297 - val_acc: 0.9920\n",
      "Epoch 17/1000\n",
      "220s - loss: 0.0131 - acc: 0.9958 - val_loss: 0.0313 - val_acc: 0.9921\n",
      "Epoch 18/1000\n",
      "225s - loss: 0.0169 - acc: 0.9952 - val_loss: 0.0438 - val_acc: 0.9912\n",
      "Epoch 19/1000\n",
      "224s - loss: 0.0192 - acc: 0.9943 - val_loss: 0.0362 - val_acc: 0.9917\n",
      "Epoch 20/1000\n",
      "226s - loss: 0.0171 - acc: 0.9950 - val_loss: 0.0379 - val_acc: 0.9908\n",
      "Epoch 21/1000\n",
      "216s - loss: 0.0131 - acc: 0.9957 - val_loss: 0.0352 - val_acc: 0.9911\n",
      "Epoch 22/1000\n",
      "213s - loss: 0.0100 - acc: 0.9969 - val_loss: 0.0557 - val_acc: 0.9885\n",
      "Epoch 23/1000\n",
      "214s - loss: 0.0113 - acc: 0.9965 - val_loss: 0.0429 - val_acc: 0.9901\n",
      "Epoch 24/1000\n",
      "228s - loss: 0.0126 - acc: 0.9962 - val_loss: 0.0631 - val_acc: 0.9882\n",
      "Epoch 25/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-150c368fe834>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m                         model.fit(X_train, y_train, epochs=1000, batch_size=32,verbose=2,callbacks=callbacks,shuffle=True, \n\u001b[1;32m---> 39\u001b[1;33m                                   validation_data=(X_test, y_test))\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_______________________________________________________________________________\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    861\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 863\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1079\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1080\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2268\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflowcpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# I just want to compare val-acc, I don't need to save model.\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "for Nfilters1 in [40, 80]:\n",
    "    for Nfilters2 in [100, 200]:\n",
    "        for kernelsize1 in [3, 5]:\n",
    "            for kernelsize2 in [3, 5]:\n",
    "                for Nunits in [1000, 2000]:\n",
    "                    for dropout in [0., 0.2, 0.5]:\n",
    "                      for batchsize in [128, 256]:\n",
    "                        model = Sequential()\n",
    "                        model.add(Conv2D(filters=Nfilters1, kernel_size=kernelsize1, padding=\"same\", input_shape=(28, 28, 1), \n",
    "                                         activation='relu'))\n",
    "                        model.add(BatchNormalization())\n",
    "                        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "                        model.add(Dropout(dropout))\n",
    "                        model.add(Conv2D(filters=Nfilters2, kernel_size=kernelsize2, padding=\"same\", activation='relu'))\n",
    "                        model.add(BatchNormalization())\n",
    "                        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "                        model.add(Dropout(dropout))\n",
    "                        model.add(Flatten())\n",
    "                        model.add(Dense(Nunits, activation='relu'))\n",
    "                        model.add(BatchNormalization())\n",
    "                        model.add(Dropout(dropout))\n",
    "                        model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "                        model.compile(optimizer='adam',\n",
    "                                      loss='categorical_crossentropy',\n",
    "                                      metrics=['accuracy'])\n",
    "\n",
    "                        callbacks = [EarlyStopping(monitor='val_acc', min_delta=0., patience=10)]\n",
    "                        \n",
    "                        print(\"Nfilters1: \"+ str(Nfilters1) +\", Nfilters2: \"+ str(Nfilters2) +\", kernelsize1: \"+ \n",
    "                              str(kernelsize1) +\", \"+ \"kernelsize2: \"+ str(kernelsize2) +\", Nunits: \"+ str(Nunits) +\n",
    "                              \", dropout: \"+ str(dropout) + \", batchsize: \"+ str(batchsize))\n",
    "\n",
    "                        model.fit(X_train, y_train, epochs=1000, batch_size=32,verbose=2,callbacks=callbacks,shuffle=True, \n",
    "                                  validation_data=(X_test, y_test))\n",
    "                        \n",
    "                        print(\"_______________________________________________________________________________\")\n",
    "                        print(\"_______________________________________________________________________________\")\n",
    "                        print(\"_______________________________________________________________________________\")\n",
    "                        \n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I made a mistake, batchsize doesn't vary. It allows me to compare two runs with the same parameters.\n",
    "\n",
    "# Nfilters1: 40, Nfilters2: 100, kernelsize1: 3, kernelsize2: 3, Nunits: 1000, dropout: 0.0, batchsize: 32\n",
    "# Best val_acc 0.9917 at 7\n",
    "# identical run: Best val_acc 0.9926 at 25\n",
    "\n",
    "# With dropout = 0.2\n",
    "# Best val_acc 0.9921 at 17\n",
    "\n",
    "# So the result is similar with dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each run takes more than a hour. I cannot do that 128 times. So I need to do a less complete search.\n",
    "\n",
    "# Let's read\n",
    "# https://arxiv.org/pdf/1512.00567.pdf\n",
    "# http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42000, 28, 28, 1)\n",
      "(42000, 10)\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/1\n",
      "79s - loss: 0.1773 - acc: 0.9464 - val_loss: 0.0846 - val_acc: 0.9736\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/1\n",
      "73s - loss: 0.0847 - acc: 0.9734 - val_loss: 0.0667 - val_acc: 0.9799\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/1\n",
      "73s - loss: 0.0650 - acc: 0.9786 - val_loss: 0.0580 - val_acc: 0.9813\n",
      "Train on 33600 samples, validate on 8400 samples\n",
      "Epoch 1/1\n",
      "72s - loss: 0.0515 - acc: 0.9838 - val_loss: 0.0820 - val_acc: 0.9788\n"
     ]
    }
   ],
   "source": [
    "# Test to compare gpu, cpu. I have gpu in the notebook tensorflowgpu.ipynb\n",
    "\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "%matplotlib inline\n",
    "\n",
    "# create the training & test sets, skipping the header row with [1:]\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "xtrain = train.drop('label',axis=1).values.reshape((42000,28,28,1))/ 255.0\n",
    "ytrain = train['label'].values\n",
    "# Maybe need to use trainLabels = np_utils.to_categorical(trainLabels, 10)\n",
    "xtest = test.values.reshape((28000,28,28,1))/ 255.0\n",
    "\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "label = to_categorical(ytrain, num_classes=10)\n",
    "print(xtrain.shape)\n",
    "print(label.shape)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from numpy import argmax\n",
    "# Split into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(xtrain, label, test_size=0.2, random_state=42)\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=20, kernel_size=3, padding=\"same\", input_shape=(28, 28, 1), \n",
    "                 activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(filters=50, kernel_size=3, padding=\"same\", activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_acc', min_delta=0., patience=10)]\n",
    "\n",
    "for batchsize in [32, 64, 128, 256]:\n",
    "    model.fit(X_train, y_train, epochs=1, batch_size=32,verbose=2,callbacks=callbacks,shuffle=True, \n",
    "              validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflowcpu]",
   "language": "python",
   "name": "conda-env-tensorflowcpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
